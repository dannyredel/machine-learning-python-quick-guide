{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Pipelines\"\n",
    "author: Daniel Redel\n",
    "date: today\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    code-fold: false\n",
    "    html-math-method: katex\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have seen that in order to achieve a final predictive model, several steps are often required. The misuse of some of these steps in connection with others have been so common in various fields of study that have occasionally triggered warnings and criticism from the\n",
    "machine learning community. \n",
    "\n",
    "In this chapter we focus primarily on **the proper implementation of such composite processes in connection with resampling evaluation rules**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea**: any held-out fold in each iteration of cross-validation, which serves as a test set for evaluating the surrogate model, must be truly treated as unseen data. In other words, the held-out fold should not be used in steps such as normalization and/or feature selection/extraction. After all, “unseen” data can naturally not be used in these steps.\n",
    "\n",
    "**The Mistake**: Insofar as feature selection and cross-validation are concerned, the most common mistake is to apply feature selection on the full data at hand, construct a classifier using selected features, and then use cross-validation to evaluate the performance of the classifier. This practice leads to selection bias, which is caused because the classifier is evaluated based on samples that were used in the first place to select features that are part of the classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
